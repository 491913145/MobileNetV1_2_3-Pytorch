{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "from mobilenetv3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small\n",
    "# net_small = mobilenetv3(mode='small')\n",
    "# state_dict = torch.load('mobilenetv3_small_67.4.pth.tar')\n",
    "# net_small.load_state_dict(state_dict)\n",
    "\n",
    "# large\n",
    "# net_large = mobilenetv3(mode='large')\n",
    "#net_large = mobilenetv3(mode='large')\n",
    "\n",
    "#x = torch.randn(2,3,224,224)\n",
    "#y = net_large(x)\n",
    "#print(y.size())\n",
    "#print(y)\n",
    "import os \n",
    "os.system(\"rm ./weights/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobilenetv3(mode='large', n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "input_size = 224\n",
    "batch_size = 16\n",
    "n_worker = 4\n",
    "\n",
    "train_dir = './traindir'\n",
    "train_datasets = datasets.ImageFolder(\n",
    "        train_dir, transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=n_worker, pin_memory=True)\n",
    "\n",
    "val_dir = './valdir'\n",
    "val_datasets = datasets.ImageFolder(\n",
    "        val_dir, transforms.Compose([\n",
    "        transforms.Resize(int(input_size/0.875)),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_datasets,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=n_worker, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====== ====== ====== epoch 0\n",
      "Save model at epoch 0\n",
      "epoch 0\n",
      "Train Loss: 0.028018, Acc: 0.745000\n",
      "Test Loss: 0.056455, Acc: 0.500000\n",
      " ====== ====== ====== epoch 1\n",
      " ====== ====== ====== epoch 2\n",
      " ====== ====== ====== epoch 3\n",
      " ====== ====== ====== epoch 4\n",
      " ====== ====== ====== epoch 5\n",
      " ====== ====== ====== epoch 6\n",
      " ====== ====== ====== epoch 7\n",
      " ====== ====== ====== epoch 8\n",
      " ====== ====== ====== epoch 9\n",
      " ====== ====== ====== epoch 10\n",
      "Save model at epoch 10\n",
      "epoch 10\n",
      "Train Loss: 0.002239, Acc: 0.990000\n",
      "Test Loss: 0.147718, Acc: 0.950000\n",
      " ====== ====== ====== epoch 11\n",
      " ====== ====== ====== epoch 12\n",
      " ====== ====== ====== epoch 13\n",
      " ====== ====== ====== epoch 14\n",
      " ====== ====== ====== epoch 15\n",
      " ====== ====== ====== epoch 16\n",
      " ====== ====== ====== epoch 17\n",
      " ====== ====== ====== epoch 18\n",
      " ====== ====== ====== epoch 19\n",
      " ====== ====== ====== epoch 20\n",
      "Save model at epoch 20\n",
      "epoch 20\n",
      "Train Loss: 0.000363, Acc: 1.000000\n",
      "Test Loss: 0.000259, Acc: 1.000000\n",
      " ====== ====== ====== epoch 21\n",
      " ====== ====== ====== epoch 22\n",
      " ====== ====== ====== epoch 23\n",
      " ====== ====== ====== epoch 24\n",
      " ====== ====== ====== epoch 25\n",
      " ====== ====== ====== epoch 26\n",
      " ====== ====== ====== epoch 27\n",
      " ====== ====== ====== epoch 28\n",
      " ====== ====== ====== epoch 29\n",
      " ====== ====== ====== epoch 30\n",
      "Save model at epoch 30\n",
      "epoch 30\n",
      "Train Loss: 0.002718, Acc: 0.990000\n",
      "Test Loss: 0.000052, Acc: 1.000000\n",
      " ====== ====== ====== epoch 31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d1c31faabbab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m#print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#train_datasets)), train_acc / (len(train_datasets))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epoch = 1000\n",
    "learning_rate = 0.0005\n",
    "save_every = 10\n",
    "log_every = 10\n",
    "\n",
    "# --------------------训练过程---------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 50, 0.1)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "Loss_list = []\n",
    "Accuracy_list = []\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    print(' ====== ====== ====== epoch {}'.format(epoch))\n",
    "    # training-----------------------------\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    model.train()\n",
    "    for batch_x, batch_y in (train_dataloader):  #tqdm\n",
    "        batch_x, batch_y = Variable(batch_x).cuda(), Variable(batch_y).cuda()\n",
    "        out = model(batch_x)\n",
    "        loss = loss_func(out, batch_y)\n",
    "        train_loss += loss.item()  #loss.data[0]\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        train_correct = (pred == batch_y).sum()\n",
    "        train_acc += train_correct.item()  #data[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(\n",
    "        #train_datasets)), train_acc / (len(train_datasets))))\n",
    "\n",
    "    # evaluation--------------------------------\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    for batch_x, batch_y in (val_dataloader):  #tqdm\n",
    "        with torch.no_grad():\n",
    "            batch_x, batch_y = Variable(batch_x).cuda(), Variable(batch_y).cuda()\n",
    "            out = model(batch_x)\n",
    "            loss = loss_func(out, batch_y)\n",
    "            eval_loss += loss.item()  #.data[0]\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = (pred == batch_y).sum()\n",
    "            eval_acc += num_correct.item()  #.data[0]\n",
    "    #print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "        #val_datasets)), eval_acc / (len(val_datasets))))\n",
    "\n",
    "    Loss_list.append(eval_loss / (len(val_datasets)))\n",
    "    Accuracy_list.append(100 * eval_acc / (len(val_datasets)))\n",
    "\n",
    "    if epoch % save_every == 0:\n",
    "        print('Save model at epoch %d'%epoch)\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), 'weights/mobilenetv3_%d.pkl'%epoch)\n",
    "\n",
    "    if epoch % log_every == 0:\n",
    "        print('epoch {}'.format(epoch))\n",
    "        #print('epoch {}, Train Loss: {:.6f}'.format((epoch), train_loss / (len(trainDataset))))\n",
    "        print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(\n",
    "                train_datasets)), train_acc / (len(train_datasets))))\n",
    "        print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "                val_datasets)), eval_acc / (len(val_datasets))))\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 7.879641532897949\n",
      "epoch: 1, loss: 6.304314404726028\n",
      "epoch: 2, loss: 5.14381268620491\n",
      "epoch: 3, loss: 4.2963544726371765\n",
      "epoch: 4, loss: 2.775220647454262\n",
      "epoch: 5, loss: 1.8251195549964905\n",
      "epoch: 6, loss: 1.2622105292975903\n",
      "epoch: 7, loss: 1.2611360363662243\n",
      "epoch: 8, loss: 0.7494088746607304\n",
      "epoch: 9, loss: 0.6811636500060558\n"
     ]
    }
   ],
   "source": [
    "total_epoch = 100\n",
    "learning_rate = 0.001\n",
    "save_every = 10\n",
    "log_every = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 训练部分\n",
    "for epoch in range(total_epoch):\n",
    "    print(' ====== ====== ====== epoch {}'.format(epoch))\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "\n",
    "        optimizer.zero_grad()        # 梯度置零，因为反向传播过程中梯度会累加上一次循环的梯度\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()                   # 反向传播后参数更新 \n",
    "        running_loss += loss.item()       # loss累加\n",
    "    print(\"    train loss: {}\".format(epoch, running_loss))\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    for batch_x, batch_y in (val_dataloader):  #tqdm\n",
    "        with torch.no_grad():\n",
    "            batch_x, batch_y = Variable(batch_x).cuda(), Variable(batch_y).cuda()\n",
    "            out = model(batch_x)\n",
    "            loss = loss_func(out, batch_y)\n",
    "            eval_loss += loss.item()  #.data[0]\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = (pred == batch_y).sum()\n",
    "            eval_acc += num_correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
