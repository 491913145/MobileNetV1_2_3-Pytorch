{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "from mobilenetv3 import *\n",
    "from mobilenetv2 import MobileNetV2\n",
    "from mobilenetv1 import MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small\n",
    "# net_small = mobilenetv3(mode='small')\n",
    "# state_dict = torch.load('mobilenetv3_small_67.4.pth.tar')\n",
    "# net_small.load_state_dict(state_dict)\n",
    "\n",
    "# large\n",
    "# net_large = mobilenetv3(mode='large')\n",
    "#net_large = mobilenetv3(mode='large')\n",
    "\n",
    "#x = torch.randn(2,3,224,224)\n",
    "#y = net_large(x)\n",
    "#print(y.size())\n",
    "#print(y)\n",
    "import os \n",
    "os.system(\"rm ./weights/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = mobilenetv3(mode='large', n_class=2)\n",
    "model = MobileNet(n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])'''\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                 std=[1, 1, 1])\n",
    "\n",
    "input_size = 224\n",
    "batch_size = 16\n",
    "n_worker = 4\n",
    "\n",
    "train_dir = './traindir'\n",
    "train_datasets = datasets.ImageFolder(\n",
    "        train_dir, transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        #normalize,\n",
    "    ]))\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=n_worker, pin_memory=True)\n",
    "\n",
    "val_dir = './valdir'\n",
    "val_datasets = datasets.ImageFolder(\n",
    "        val_dir, transforms.Compose([\n",
    "        transforms.Resize(int(input_size/0.875)),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        #normalize,\n",
    "    ]))\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_datasets,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=n_worker, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch 0\n",
      "epoch 0\n",
      "Train Loss: 0.021677, Acc: 0.825000\n",
      "Test Loss: 0.131395, Acc: 0.500000\n",
      "epoch 5\n",
      "Train Loss: 0.003994, Acc: 0.980000\n",
      "Test Loss: 0.056103, Acc: 0.725000\n",
      "Save model at epoch 10\n",
      "epoch 10\n",
      "Train Loss: 0.005426, Acc: 0.965000\n",
      "Test Loss: 0.004147, Acc: 0.975000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f620bbb9f9a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#tqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epoch = 101\n",
    "learning_rate = 0.001\n",
    "save_every = 10\n",
    "log_every = 5\n",
    "\n",
    "# --------------------训练过程---------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 50, 0.1)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "Loss_list = []\n",
    "Accuracy_list = []\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    #print(' ====== ====== ====== epoch {}'.format(epoch))\n",
    "    # training-----------------------------\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    model.train()\n",
    "    for batch_x, batch_y in (train_dataloader):  #tqdm\n",
    "        batch_x, batch_y = Variable(batch_x).cuda(), Variable(batch_y).cuda()\n",
    "        out = model(batch_x)\n",
    "        loss = loss_func(out, batch_y)\n",
    "        train_loss += loss.item()  #loss.data[0]\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        train_correct = (pred == batch_y).sum()\n",
    "        train_acc += train_correct.item()  #data[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(\n",
    "        #train_datasets)), train_acc / (len(train_datasets))))\n",
    "\n",
    "    # evaluation--------------------------------\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    for batch_x, batch_y in (val_dataloader):  #tqdm\n",
    "        with torch.no_grad():\n",
    "            batch_x, batch_y = Variable(batch_x).cuda(), Variable(batch_y).cuda()\n",
    "            out = model(batch_x)\n",
    "            loss = loss_func(out, batch_y)\n",
    "            eval_loss += loss.item()  #.data[0]\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = (pred == batch_y).sum()\n",
    "            eval_acc += num_correct.item()  #.data[0]\n",
    "    #print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "        #val_datasets)), eval_acc / (len(val_datasets))))\n",
    "\n",
    "    Loss_list.append(eval_loss / (len(val_datasets)))\n",
    "    Accuracy_list.append(100 * eval_acc / (len(val_datasets)))\n",
    "\n",
    "    if epoch % save_every == 0:\n",
    "        print('Save model at epoch %d'%epoch)\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), 'weights/mobilenet_%d.pkl'%epoch)\n",
    "        #torch.save(model, 'weights/mobilenetv2_%d.pkl'%epoch)\n",
    "\n",
    "    if epoch % log_every == 0:\n",
    "        print('epoch {}'.format(epoch))\n",
    "        #print('epoch {}, Train Loss: {:.6f}'.format((epoch), train_loss / (len(trainDataset))))\n",
    "        print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(\n",
    "                train_datasets)), train_acc / (len(train_datasets))))\n",
    "        print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "                val_datasets)), eval_acc / (len(val_datasets))))\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "transf = transforms.Compose([\n",
    "         transforms.Resize(256),\n",
    "         transforms.CenterCrop(224),\n",
    "         transforms.ToTensor(),\n",
    "         #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "         #                     std=[0.229, 0.224, 0.225]),\n",
    "         ])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    image = transf(image).unsqueeze(0)\n",
    "    #print(np.array(image)[0][0][0][:10])\n",
    "    return image.cuda()\n",
    "\n",
    "def forward(image_name):\n",
    "    inp = image_loader(image_name)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(inp).cpu()\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7085, -1.6978]])\n",
      "tensor([[ 1.9104, -1.9086]])\n",
      "tensor([[ 0.6705, -0.6266]])\n",
      "tensor([[ 1.8742, -1.8742]])\n",
      "tensor([[-3.5147,  3.2813]])\n",
      "tensor([[-3.7945,  3.5505]])\n",
      "tensor([[-4.2304,  3.9035]])\n",
      "tensor([[-2.9786,  2.7847]])\n"
     ]
    }
   ],
   "source": [
    "forward(\"valdir/greenlight/000771_4.jpg\")\n",
    "forward(\"valdir/greenlight/000774_3.jpg\")\n",
    "forward(\"valdir/greenlight/000849_2.jpg\")\n",
    "forward(\"valdir/greenlight/000840_2.jpg\")\n",
    "forward(\"valdir/pointer/000650_1.jpg\")\n",
    "forward(\"valdir/pointer/000656_1.jpg\")\n",
    "forward(\"valdir/pointer/000692_2.jpg\")\n",
    "forward(\"valdir/pointer/000722_1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.45490196 0.43529412 0.43137255 ... 0.45490196 0.45490196 0.4627451 ]\n",
      "  [0.46666667 0.44313726 0.43137255 ... 0.4745098  0.46666667 0.4745098 ]\n",
      "  [0.45882353 0.4392157  0.42745098 ... 0.47843137 0.4745098  0.4862745 ]\n",
      "  ...\n",
      "  [0.1254902  0.09411765 0.0627451  ... 0.07058824 0.10196079 0.07450981]\n",
      "  [0.1254902  0.09411765 0.05882353 ... 0.07058824 0.11372549 0.08627451]\n",
      "  [0.11764706 0.08235294 0.05490196 ... 0.07450981 0.1254902  0.10196079]]\n",
      "\n",
      " [[0.49803922 0.47843137 0.4745098  ... 0.5137255  0.5137255  0.52156866]\n",
      "  [0.50980395 0.4862745  0.4745098  ... 0.53333336 0.5254902  0.53333336]\n",
      "  [0.5019608  0.48235294 0.47058824 ... 0.5372549  0.53333336 0.54509807]\n",
      "  ...\n",
      "  [0.1254902  0.10196079 0.07843138 ... 0.14509805 0.17254902 0.14117648]\n",
      "  [0.1254902  0.09411765 0.07450981 ... 0.14117648 0.18431373 0.15686275]\n",
      "  [0.11764706 0.08627451 0.07058824 ... 0.14117648 0.19607843 0.17254902]]\n",
      "\n",
      " [[0.4745098  0.45490196 0.44705883 ... 0.49411765 0.49411765 0.5019608 ]\n",
      "  [0.4862745  0.4627451  0.4509804  ... 0.5137255  0.5058824  0.5137255 ]\n",
      "  [0.47843137 0.45882353 0.44705883 ... 0.5176471  0.5137255  0.5254902 ]\n",
      "  ...\n",
      "  [0.15686275 0.13333334 0.10196079 ... 0.16862746 0.19607843 0.15686275]\n",
      "  [0.15686275 0.1254902  0.09803922 ... 0.16078432 0.2        0.16862746]\n",
      "  [0.14901961 0.11764706 0.09411765 ... 0.16470589 0.21176471 0.18431373]]]\n"
     ]
    }
   ],
   "source": [
    "ttt = Image.open(\"valdir/greenlight/000771_4.jpg\").convert('RGB')\n",
    "\n",
    "trans  = transforms.Compose([\n",
    "         transforms.Resize(224),\n",
    "         transforms.CenterCrop(224),\n",
    "         transforms.ToTensor(),\n",
    "         ])\n",
    "ttt = trans(ttt)\n",
    "print(np.array(ttt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
